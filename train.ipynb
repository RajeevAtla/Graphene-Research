{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajeevAtla/Superconductivity-Research/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqVBkgxfiFIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a04b63b-8afd-444c-cbfa-d34e62b94de8"
      },
      "source": [
        "!pip install tf-nightly imageio matplotlib numpy pathlib google IPython pandas scikit-learn --upgrade\n",
        "print( \"Done installing packages!\" )\n",
        "#To make sure user has the most up to date packages before rest of program runs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.4.0.dev20200807)\n",
            "Requirement already up-to-date: imageio in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.6/dist-packages (3.3.0)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.19.1)\n",
            "Requirement already up-to-date: pathlib in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already up-to-date: google in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already up-to-date: IPython in /usr/local/lib/python3.6/dist-packages (7.16.1)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.23.2)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers>=1.12 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0.dev2020080701)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tb-nightly<3.0.0a0,>=2.4.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0a20200807)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from IPython) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from IPython) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: backcall in /usr/local/lib/python3.6/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython) (4.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from IPython) (0.17.2)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->IPython) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Done installing packages!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuVuAH4FMcLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2d0ca57-de4a-4803-90b9-1b8c159d7700"
      },
      "source": [
        "#load all the packages\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import pathlib\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import pandas as pd\n",
        "import functools\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "#21, 263\n",
        "#batch = 11\n",
        "\n",
        "print( \"Done importing packages!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done importing packages!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsMEaZ6tNrAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f73a5f1c-a7be-4b15-f602-703c461c515b"
      },
      "source": [
        "#import data using pandas\n",
        "original_data = pd.read_csv( 'https://raw.githubusercontent.com/RajeevAtla/Superconductivity-Dataset/master/dataset.csv', sep = ',', header = 0 )\n",
        "#I've hosted the data in a Github repo\n",
        "#This above function will extract it\n",
        "\n",
        "material_id = original_data.pop( 'material' ) #Material name is irrelevant for our purposes\n",
        "critical_temp = original_data.pop( 'critical_temp' ) #So is critical temperature\n",
        "\n",
        "print( original_data.shape ) #To be able to make sense of other stuff\n",
        "\n",
        "print( \"Done importing data!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21263, 81)\n",
            "Done importing data!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTeB3pZEMnId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cd1c9dd-8217-454c-c5f5-261a31eae52e"
      },
      "source": [
        "split_seed = 44 #random seed for reproducibiliy\n",
        "split_ratio = 0.2 #specifically the ratio of test data to train data\n",
        "\n",
        "train, test = train_test_split( original_data, test_size = split_ratio, random_state = split_seed )\n",
        "\n",
        "print( \"Done splitting the data!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done splitting the data!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e51bJl17LynK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "312612e8-83d5-4a02-a694-f90ce27c0e0b"
      },
      "source": [
        "train_t = tf.constant(train.to_numpy())\n",
        "test_t = tf.constant(test.to_numpy())\n",
        "\n",
        "print(train_t)\n",
        "\n",
        "train_tensor = tf.data.Dataset.from_tensors(train_t)\n",
        "test_tensor = tf.data.Dataset.from_tensors(test_t)\n",
        "\n",
        "print(train_tensor)\n",
        "\n",
        "batch_size = 11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  5.          89.33718     53.24573462 ...   0.96153846   0.8\n",
            "    0.42307692]\n",
            " [  5.          52.14440704  60.47589305 ...   1.235        1.32664992\n",
            "    1.11345184]\n",
            " [  5.          80.80661     48.14163596 ...   0.93846154   1.6\n",
            "    0.77760823]\n",
            " ...\n",
            " [  1.         118.71       118.71       ...   0.           0.\n",
            "    0.        ]\n",
            " [  5.          79.40045     51.50115462 ...   1.04         0.8\n",
            "    0.32364785]\n",
            " [  3.          96.84166667  67.58421053 ...   2.92105263   1.41421356\n",
            "    0.8089698 ]], shape=(17010, 81), dtype=float64)\n",
            "<TensorDataset shapes: (17010, 81), types: tf.float64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiF7FBW4jj0C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "09854650-fd57-4919-f5a9-5f18d6e00310"
      },
      "source": [
        "'''\n",
        "\n",
        "shuffle_seed = 123421\n",
        "shuffle_buffer_size = 9 #max to get the best shuffling\n",
        "\n",
        "DATA_URL = 'https://raw.githubusercontent.com/RajeevAtla/Superconductivity-Dataset/master/dataset_trial2.csv'\n",
        "file_path = tf.keras.utils.get_file( \"dataset2.csv\", DATA_URL )\n",
        "\n",
        "print(file_path)\n",
        "\n",
        "\n",
        "data_tensor = tf.data.experimental.make_csv_dataset( file_path, batch_size = batch_size, shuffle_seed = shuffle_seed, shuffle_buffer_size = shuffle_buffer_size, header = True, select_columns = np.ndarray.tolist( np.arange( 1, 82 ) ) )\n",
        "\n",
        "print(\"Done making data pipeline!\")\n",
        "\n",
        "#list(data_tensor.as_numpy_iterator())\n",
        "\n",
        "#for element in data_tensor:\n",
        "  #print(element)\n",
        "\n",
        "print(data_tensor)\n",
        "\n",
        "#print(data_tensor.element_spec)\n",
        "\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nbatch_size = 11\\nshuffle_seed = 123421\\nshuffle_buffer_size = 9 #max to get the best shuffling\\n\\nDATA_URL = \\'https://raw.githubusercontent.com/RajeevAtla/Superconductivity-Dataset/master/dataset_trial2.csv\\'\\nfile_path = tf.keras.utils.get_file( \"dataset2.csv\", DATA_URL )\\n\\nprint(file_path)\\n\\n\\ndata_tensor = tf.data.experimental.make_csv_dataset( file_path, batch_size = batch_size, shuffle_seed = shuffle_seed, shuffle_buffer_size = shuffle_buffer_size, header = True, select_columns = np.ndarray.tolist( np.arange( 1, 82 ) ) )\\n\\nprint(\"Done making data pipeline!\")\\n\\n#list(data_tensor.as_numpy_iterator())\\n\\n#for element in data_tensor:\\n  #print(element)\\n\\nprint(data_tensor)\\n\\n#print(data_tensor.element_spec)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGrAC4BTBi9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_tensor_unbatch = data_tensor.unbatch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIv7xNnksj33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxWJ9U-jPkir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20436ce6-29af-42fe-eacd-8fdc46507bc3"
      },
      "source": [
        "def make_discriminator( ):\n",
        "\n",
        "    model = keras.Sequential( )\n",
        "    model.add( keras.Input( 81, ) )\n",
        "    print ( model.output_shape )\n",
        "\n",
        "    model.add( layers.Dense( 81, ) )\n",
        "    print ( model.output_shape )\n",
        "    model.add( layers.BatchNormalization( ) )\n",
        "    model.add( layers.LeakyReLU( ) )\n",
        "\n",
        "    model.add( layers.Dense( 64, ) )\n",
        "    print ( model.output_shape )\n",
        "    model.add( layers.BatchNormalization( ) )\n",
        "    model.add( layers.LeakyReLU( ) )\n",
        "\n",
        "    model.add( layers.Dense( 16, ) )\n",
        "    print ( model.output_shape )\n",
        "    model.add( layers.BatchNormalization() )\n",
        "    model.add( layers.LeakyReLU( ) )\n",
        "\n",
        "    model.add( layers.Dense( 4, ) )\n",
        "    print ( model.output_shape )\n",
        "    model.add( layers.BatchNormalization( ) )\n",
        "    model.add( layers.LeakyReLU( ) )\n",
        "\n",
        "    model.add( layers.Dense( 1, ) )\n",
        "    print ( model.output_shape )\n",
        "    model.add( layers.BatchNormalization( ) )\n",
        "    model.add( layers.LeakyReLU( ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "print( \"Done creating function to create generator!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done creating function to create generator!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZHGRSU-rME1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "86a158ba-b453-4229-8a2a-d4499dd8b9b4"
      },
      "source": [
        "discriminator = make_discriminator( )\n",
        "print( \"Done creating discriminator!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 81)\n",
            "(None, 81)\n",
            "(None, 64)\n",
            "(None, 16)\n",
            "(None, 4)\n",
            "(None, 1)\n",
            "Done creating discriminator!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLiA23OTNwE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70778d56-149a-439f-c857-3bbdd479dc63"
      },
      "source": [
        "def make_generator( ):\n",
        "    model = keras.Sequential( )\n",
        "    model.add( keras.Input( 81, ) )\n",
        "    print ( model.output_shape )\n",
        "\n",
        "    model.add( layers.Dense( 128, ) )\n",
        "    print ( model.output_shape )\n",
        "    model.add( layers.BatchNormalization( ) )\n",
        "    model.add( layers.LeakyReLU( ) )\n",
        "\n",
        "    model.add( layers.Dense( 81, ) )\n",
        "    print ( model.output_shape )\n",
        "    model.add( layers.BatchNormalization( ) )\n",
        "    model.add( layers.LeakyReLU( ) )\n",
        "\n",
        "    model.add( layers.Reshape( ( 81, ) ) )\n",
        "    print ( model.output_shape )\n",
        "\n",
        "    return model\n",
        "\n",
        "print( \"Done creating function to create generator!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done creating function to create generator!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn7Tpgg-NxL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e3c584c7-2270-4ba3-91b2-75a52db089e2"
      },
      "source": [
        "generator = make_generator( )\n",
        "print( \"Done creating generator!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 81)\n",
            "(None, 128)\n",
            "(None, 81)\n",
            "(None, 81)\n",
            "Done creating generator!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoG_C2hYJiK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fbc856a-bda0-4246-9d91-b8251f6f8ce3"
      },
      "source": [
        "pdb on"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4HJKxa7n6sc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a61079c-4473-4e2f-8c6c-dc1dd40b0f42"
      },
      "source": [
        "cross_entropy = keras.losses.BinaryCrossentropy( from_logits = True )\n",
        "print( \"Done defining cross entropy!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining cross entropy!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBO16WCmfFwc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58d39a12-7455-4dc0-e261-ca3f4f731839"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy( tf.ones_like(real_output), real_output )\n",
        "    fake_loss = cross_entropy( tf.zeros_like(fake_output), fake_output )\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "print(\"Done defining discriminator loss function!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining discriminator loss function!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgB8xheAhGEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2f6199e-f4db-4392-9970-3466d3c2202a"
      },
      "source": [
        "def generator_loss( fake_output ):\n",
        "    return cross_entropy( tf.ones_like( fake_output ), fake_output )\n",
        "  \n",
        "print( \"Done defining generator loss function!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining generator loss function!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yar9n33nhdxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d0a4f10-b535-4f49-f183-6bcbcf1aeefd"
      },
      "source": [
        "generator_optimizer = keras.optimizers.Adam( 1e-4 )\n",
        "discriminator_optimizer = keras.optimizers.Adam( 1e-4 )\n",
        "\n",
        "print( \"Done defining the optimizers for both the generator and discriminator!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining the optimizers for both the generator and discriminator!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXod457ohqTb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb251fcb-ad53-4e0d-fba7-b0b0a915c43d"
      },
      "source": [
        "#A checkpoint, just in case things go wrong\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join( checkpoint_dir, \"ckpt\" )\n",
        "checkpoint = tf.train.Checkpoint( generator_optimizer = generator_optimizer, discriminator_optimizer = discriminator_optimizer, generator = generator, discriminator = discriminator )\n",
        "\n",
        "print( \"Done saving checkpoint!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done saving checkpoint!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pokb9lXBh37M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7502e8c-8f38-4e4f-cd09-923cc9fead2c"
      },
      "source": [
        "EPOCHS = 1000\n",
        "noise_dim = 81\n",
        "num_examples_to_generate = 16\n",
        "print( \"Done defining hyperparameters!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining hyperparameters!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e2j-048ofsv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2135c3a9-1082-4ba2-c58d-908d31ea8c2b"
      },
      "source": [
        "seed = tf.random.normal( [ num_examples_to_generate, noise_dim ] )\n",
        "\n",
        "print( \"Done defining the seed\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining the seed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsRY0VhEQLYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def time_elapsed( start ):\n",
        "  elapsed_time = time.time() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6w3j9KdknNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3403077-9560-4b42-dc02-0da6aa149c4a"
      },
      "source": [
        "@tf.function\n",
        "def train_step( data ):\n",
        "    noise = tf.random.normal( [ batch_size, noise_dim ] )\n",
        "\n",
        "    with tf.GradientTape( ) as gen_tape, tf.GradientTape( ) as disc_tape:\n",
        "      generated_data = generator( noise, training = True )\n",
        "\n",
        "      real_output = discriminator( data, training = True )\n",
        "      fake_output = discriminator( generated_data, training = True )\n",
        "\n",
        "      gen_loss = generator_loss( fake_output )\n",
        "      disc_loss = discriminator_loss( real_output, fake_output )\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient( gen_loss, generator.trainable_variables )\n",
        "    gradients_of_discriminator = disc_tape.gradient( disc_loss, discriminator.trainable_variables )\n",
        "\n",
        "    generator_optimizer.apply_gradients( zip ( gradients_of_generator, generator.trainable_variables ) )\n",
        "    discriminator_optimizer.apply_gradients( zip ( gradients_of_discriminator, discriminator.trainable_variables ) )\n",
        "\n",
        "print( \"Done defining a training step!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining a training step!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbjGSwvBolxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5fbc9cd-01f1-4e50-f87e-5a6b20b2c0a7"
      },
      "source": [
        "def train( dataset, epochs ):\n",
        "  for epoch in range( epochs ):\n",
        "    start = time.time( )\n",
        "\n",
        "    for batch in dataset:\n",
        "      train_step( batch )\n",
        "\n",
        "  \n",
        "    display.clear_output( wait = True )\n",
        "    generate_and_save_data( generator, epoch + 1, seed )\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if ( epoch + 1 ) % 15 == 0:\n",
        "      checkpoint.save( file_prefix = checkpoint_prefix )\n",
        "\n",
        "    #print('Time for epoch { } is { } sec'.format( epoch + 1, time.time( ) - start ) )\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output( wait = True )\n",
        "  generate_and_save_data( generator, epochs, seed )\n",
        "\n",
        "print( \"Done defining training loop!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining training loop!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCzZs9RByANZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b74179a9-c286-4614-bdce-98f9587b637f"
      },
      "source": [
        "def generate_and_save_data( model, epoch, test_input ):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "\n",
        "  predictions = model( test_input, training = False )\n",
        "\n",
        "print( \"Done defining function to generate data!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done defining function to generate data!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUIibg2vyFkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90d1fdac-fc13-423e-b69a-b027bf85c825"
      },
      "source": [
        "train( train_tensor, EPOCHS )\n",
        "\n",
        "print( \"Done training model!\" ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done training model!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lLLz0_dyNIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c9e7a4c-e36b-488f-ed3d-9638158bdd57"
      },
      "source": [
        "checkpoint.restore( tf.train.latest_checkpoint( checkpoint_dir ) )\n",
        "print( \"Done restoring last checkpoint!\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done restoring last checkpoint!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}